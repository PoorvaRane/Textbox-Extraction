{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The architecture of our CNN is given in Figure 1. The structure\n",
    "# can be summarized as 10×10x1−26×26×4−100−M,\n",
    "# where M is the number of classes. The input is a grayscale\n",
    "# image patch. The size of the image patch is 28×28 pixels. Our\n",
    "# CNN architecture contains only one convolution layer which\n",
    "# consists of 4 kernels. The size of each kernel is 3 × 3 pixels.\n",
    "# Unlike other traditional CNN architecture, the pooling layer is\n",
    "# not used in our architecture. Then one fully connected layer\n",
    "# of 100 neurons follows the convolution layer. The last layer\n",
    "# consists of a logistic regression with softmax which outputs\n",
    "# the probability of each class, such that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get train data\n",
    "'''\n",
    "# training_data = pickle.load(open('training_data.pkl','r'))\n",
    "training_data = pickle.load(open('train_data_new.pkl','r'))\n",
    "\n",
    "training_data = np.array(training_data)\n",
    "np.random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Get Validation data\n",
    "'''\n",
    "# validation_data = pickle.load(open('validation_data.pkl','r'))\n",
    "validation_data = pickle.load(open('validation_data_new.pkl','r'))\n",
    "\n",
    "validation_data = np.array(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Get Test data\n",
    "'''\n",
    "# test_data = pickle.load(open('test_data.pkl','r'))\n",
    "test_data = pickle.load(open('test_data_new.pkl','r'))\n",
    "\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self): # DO NOT HARDCODE\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel 10x10, 4 output channels, 3x3 square convolution\n",
    "        self.conv1 = nn.Conv2d(1, 4, 3)\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "        # an affine operation: y = Wx + b\n",
    "#         self.fc1 = nn.Linear(4 * 8 * 8, 100)\n",
    "        self.fc1 = nn.Linear(4 * 12 * 12, 100)\n",
    "        self.fc2 = nn.Linear(100, 1) #Number of classes = 'text'\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "#         x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        out = self.sigmoid(x)\n",
    "#         out = F.log_softmax(x)\n",
    "        return out\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "trainloader = DataLoader(training_data.tolist(), batch_size=1, shuffle=True)\n",
    "validloader = DataLoader(validation_data.tolist())\n",
    "testloader = DataLoader(test_data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_function(validloader, net, optimizer, criterion):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()\n",
    "    for i, data in enumerate(validloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels_var = Variable(inputs.unsqueeze(0).float(), volatile=True), Variable(labels.float(), volatile=True)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels_var.unsqueeze(-1))\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        predicted = torch.ge(outputs.data, torch.FloatTensor([0.5])) # Because batch size is 1\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.long() == labels).sum()\n",
    "    \n",
    "    print(\"******validation*** running_loss = \",running_loss/float(i+1))\n",
    "    print('Accuracy of the network on the 1728 validation images: %d %%' % (100 * correct / total))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_function(testloader, net, optimizer, criterion):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()\n",
    "    predicted_list = []\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "#         inputs, labels_var = Variable(inputs.unsqueeze(0).float()), Variable(labels.long())\n",
    "        inputs, labels_var = Variable(inputs.unsqueeze(0).float()), Variable(labels.float())\n",
    "\n",
    "        outputs = net(inputs)\n",
    "#         loss = criterion(outputs, labels_var)\n",
    "        loss = criterion(outputs, labels_var.unsqueeze(-1))\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "\n",
    "        predicted = torch.ge(outputs.data, torch.FloatTensor([0.5])) # Because batch size is 1\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_list.append(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.long() == labels).sum()\n",
    "    \n",
    "    print(\"******test*** running_loss = \",running_loss/float(i+1))\n",
    "    print('Accuracy of the network on the 10368 test images: %d %%' % (100 * correct / total))\n",
    "    return predicted_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function(trainloader, net, optimizer, criterion):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.train()\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # wrap them in Variable\n",
    "        inputs, labels_var = Variable(inputs.unsqueeze(0).float()), Variable(labels.float())\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels_var.unsqueeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data.numpy()[0]\n",
    "    \n",
    "        predicted = torch.ge(outputs.data, torch.FloatTensor([0.5])) # Because batch size is 1\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.long() == labels).sum()\n",
    "    print(\"****** running_loss = \",running_loss/float(i+1))\n",
    "\n",
    "    print('Accuracy of the network on the 21643 train images: %d %%' % (100 * correct / total))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** running_loss =  0.670638093567\n",
      "Accuracy of the network on the 21643 train images: 61 %\n",
      "******validation*** running_loss =  0.64688098778\n",
      "Accuracy of the network on the 1728 validation images: 65 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.651444816306\n",
      "Accuracy of the network on the 21643 train images: 63 %\n",
      "******validation*** running_loss =  0.624656138411\n",
      "Accuracy of the network on the 1728 validation images: 66 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.635574142445\n",
      "Accuracy of the network on the 21643 train images: 65 %\n",
      "******validation*** running_loss =  0.597281337315\n",
      "Accuracy of the network on the 1728 validation images: 70 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.621352197161\n",
      "Accuracy of the network on the 21643 train images: 66 %\n",
      "******validation*** running_loss =  0.581506395484\n",
      "Accuracy of the network on the 1728 validation images: 72 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.60116117146\n",
      "Accuracy of the network on the 21643 train images: 68 %\n",
      "******validation*** running_loss =  0.521261279512\n",
      "Accuracy of the network on the 1728 validation images: 76 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.574233638773\n",
      "Accuracy of the network on the 21643 train images: 71 %\n",
      "******validation*** running_loss =  0.522757867225\n",
      "Accuracy of the network on the 1728 validation images: 74 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.538255133505\n",
      "Accuracy of the network on the 21643 train images: 73 %\n",
      "******validation*** running_loss =  0.4555280197\n",
      "Accuracy of the network on the 1728 validation images: 78 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.494856338991\n",
      "Accuracy of the network on the 21643 train images: 76 %\n",
      "******validation*** running_loss =  0.406533516681\n",
      "Accuracy of the network on the 1728 validation images: 81 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.458307856249\n",
      "Accuracy of the network on the 21643 train images: 78 %\n",
      "******validation*** running_loss =  0.397370842323\n",
      "Accuracy of the network on the 1728 validation images: 81 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.436608501488\n",
      "Accuracy of the network on the 21643 train images: 79 %\n",
      "******validation*** running_loss =  0.396710347997\n",
      "Accuracy of the network on the 1728 validation images: 81 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.404155836325\n",
      "Accuracy of the network on the 21643 train images: 81 %\n",
      "******validation*** running_loss =  0.355771252037\n",
      "Accuracy of the network on the 1728 validation images: 83 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.37773396822\n",
      "Accuracy of the network on the 21643 train images: 82 %\n",
      "******validation*** running_loss =  0.342430637528\n",
      "Accuracy of the network on the 1728 validation images: 85 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.381416248484\n",
      "Accuracy of the network on the 21643 train images: 82 %\n",
      "******validation*** running_loss =  0.332901999539\n",
      "Accuracy of the network on the 1728 validation images: 85 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.355951771662\n",
      "Accuracy of the network on the 21643 train images: 83 %\n",
      "******validation*** running_loss =  0.349368352932\n",
      "Accuracy of the network on the 1728 validation images: 84 %\n",
      "-----------------------------\n",
      "****** running_loss =  0.364195786077\n",
      "Accuracy of the network on the 21643 train images: 83 %\n",
      "******validation*** running_loss =  0.351702672087\n",
      "Accuracy of the network on the 1728 validation images: 84 %\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Training Phase\n",
    "net = Net()\n",
    "# criterion =  nn.NLLLoss() #nn.CrossEntropyLoss()\n",
    "criterion =  nn.BCELoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    train_function(trainloader, net, optimizer, criterion)\n",
    "    validation_function(validloader, net, optimizer, criterion)\n",
    "#     predicted_list = test_function(testloader, net, optimizer, criterion)\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-50b3a7acc8e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnew_pr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicted_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnew_pr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnew_pr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_pr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_list' is not defined"
     ]
    }
   ],
   "source": [
    "# IGNORE THE CODE FROM HERE\n",
    "\n",
    "new_pr = []\n",
    "for pr in predicted_list:\n",
    "    new_pr.append(pr[0])\n",
    "new_pr = np.array(new_pr) \n",
    "new_pr = new_pr.reshape(12,36,24)\n",
    "sample= np.array(new_pr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6121809637888141"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(predicted_list) - np.count_nonzero(y_Test))/float(len(predicted_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ", ...,\n",
       "        \n",
       " 1\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 1\n",
       "[torch.LongTensor of size 1]\n",
       ", \n",
       " 1\n",
       "[torch.LongTensor of size 1]\n",
       "],\n",
       "       [\n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 1\n",
       "[torch.LongTensor of size 1]\n",
       ", ...,\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 1\n",
       "[torch.LongTensor of size 1]\n",
       ", \n",
       " 1\n",
       "[torch.LongTensor of size 1]\n",
       "],\n",
       "       [\n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ", ...,\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ", \n",
       " 1\n",
       "[torch.LongTensor of size 1]\n",
       "],\n",
       "       ..., \n",
       "       [\n",
       " 1\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ", ...,\n",
       "        \n",
       " 1\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ", \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       "],\n",
       "       [\n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ", ...,\n",
       "        \n",
       " 1\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ", \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       "],\n",
       "       [\n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 1\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ", ...,\n",
       "        \n",
       " 1\n",
       "[torch.LongTensor of size 1]\n",
       ",\n",
       "        \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       ", \n",
       " 0\n",
       "[torch.LongTensor of size 1]\n",
       "]], dtype=object)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_list = np.array(predicted_list)\n",
    "predicted_list = predicted_list.reshape(12,864)\n",
    "predicted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALYAAAD8CAYAAADaM14OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d0627d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample, interpolation='nearest', cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAD8CAYAAACFK0QrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1MxVKpVqAKVmrlKpVAMoNXOVSqUaQP0/wENMqN8D7vEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12db8fad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "f, axarr = plt.subplots(20, 20)\n",
    "for i in range(400):\n",
    "    axarr[i/20,i%20].imshow(im_values[i+30], interpolation='bilinear', cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im_values = []\n",
    "for pred in predicted_list:\n",
    "    val = pred.numpy()\n",
    "    if val[0] == 1:\n",
    "        val[0] = 255\n",
    "        im = np.full((5, 5), 0)\n",
    "    else:\n",
    "        val[0] = 1\n",
    "        im = np.full((5, 5), 255)\n",
    "    \n",
    "    im_values.append(im)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = pickle.load(open('training_data.pkl','r'))\n",
    "\n",
    "training_data = np.array(training_data)\n",
    "\n",
    "X_Train = training_data[:,0]\n",
    "y_Train = training_data[:,1]\n",
    "\n",
    "N = len(X_Train)\n",
    "H = X_Train[0].shape[0]\n",
    "W = X_Train[0].shape[1]\n",
    "trainTensor = torch.LongTensor(N, H, W)\n",
    "for i in range(N):\n",
    "    trainTensor[i] = torch.LongTensor(X_Train[i].tolist())\n",
    "trainTensor = trainTensor.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[109, 106, 105, 100,  98,  98,  98,  94,  95,  90],\n",
       "       [ 38,  48,  44,  32,  38,  35,  36,  29,  33,  29],\n",
       "       [ 24,  22,  24,  24,  21,  22,  22,  20,  19,  21],\n",
       "       [  8,   8,   7,   8,  10,  10,   8,   8,   9,   8],\n",
       "       [ 23,  21,  21,   8,  19,   3,  24,  30,  26,  22],\n",
       "       [  7,   7,   5,   4,   7,   9,   8,   7,   6,   7],\n",
       "       [  1,   0,   0,   0,   1,   7,   2,   2,   3,   1],\n",
       "       [ 13,   7,   3,   4,   2,   7,  11,   4,   9,   9],\n",
       "       [ 18,  23,  36,  18,  28,  28,  37,  47,  39,  50],\n",
       "       [ 24,  35,  31,  51,  10,  55,  66,  77,  67,  61]], dtype=uint8)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  109  106  105  100   98   98   98   94   95   90\n",
       "   38   48   44   32   38   35   36   29   33   29\n",
       "   24   22   24   24   21   22   22   20   19   21\n",
       "    8    8    7    8   10   10    8    8    9    8\n",
       "   23   21   21    8   19    3   24   30   26   22\n",
       "    7    7    5    4    7    9    8    7    6    7\n",
       "    1    0    0    0    1    7    2    2    3    1\n",
       "   13    7    3    4    2    7   11    4    9    9\n",
       "   18   23   36   18   28   28   37   47   39   50\n",
       "   24   35   31   51   10   55   66   77   67   61\n",
       "[torch.ByteTensor of size 1x10x10]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, label = dataiter.next()\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,0 ,.,.) = \n",
       "  244  247  238  233  232  218  229  228  214  231\n",
       "  229  239  245  212  216  228  212  216  210  222\n",
       "  226  225  233  219  236  222  233  235  234  229\n",
       "  238  233  242  232  235  227  233  229  230  232\n",
       "  240  237  226  229  226  234  208  234  218  231\n",
       "  231  226  232  231  214  219  228  238  223  217\n",
       "  191  212  191  226  214  159  186  220  211  213\n",
       "  236  232  227  226  217  185  223  208  216  208\n",
       "  229  224  228  233  227  219  226  208  217  208\n",
       "  221  230  217  221  232  225  232  220  202  215\n",
       "[torch.FloatTensor of size 1x1x10x10]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 1\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Variable(torch.arange(0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_t= Variable(torch.randn(3, 5), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3829  0.5494 -1.1376 -0.6053 -1.3948\n",
       " 0.0361 -0.4575  1.0598 -0.5977 -2.6812\n",
       "-1.4205 -1.7733 -0.0889  0.7921 -0.2184\n",
       "[torch.FloatTensor of size 3x5]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
